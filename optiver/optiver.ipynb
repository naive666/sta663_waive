{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6540b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import datetime as dt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../input/optiver-realized-volatility-prediction'\n",
    "# read book data\n",
    "book_train = pd.read_parquet(os.path.join(root_path, 'book_train.parquet'))\n",
    "book_test = pd.read_parquet(os.path.join(root_path, 'book_test.parquet'))\n",
    "trade_train = pd.read_parquet(os.path.join(root_path, 'trade_train.parquet'))\n",
    "trade_test = pd.read_parquet(os.path.join(root_path, 'trade_test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(root_path, 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(root_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0da534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def cal_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Remember that log(x / y) = log(x) - log(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get group stats for different windows (seconds in bucket)\n",
    "def get_stats_window(df, seconds_in_bucket, create_feature_dict, add_suffix = False):\n",
    "    # Group by the window\n",
    "    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "    # Add a suffix to differentiate windows\n",
    "    if add_suffix:\n",
    "        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "    return df_feature\n",
    "\n",
    "def book_preprocessor(book_df, stock_id):\n",
    "    book_df['wap1'] = cal_wap1(book_df)\n",
    "    book_df['wap2'] = cal_wap2(book_df)\n",
    "    book_df['wap3'] = calc_wap3(book_df)\n",
    "    book_df['wap4'] = calc_wap4(book_df)\n",
    "    # Calculate log returns\n",
    "    book_df['log_return1'] = book_df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book_df['log_return2'] = book_df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    book_df['wap_balance'] = abs(book_df['wap1'] - book_df['wap2'])\n",
    "    # Calculate spread\n",
    "    book_df['price_spread'] = (book_df['ask_price1'] - book_df['bid_price1']) / ((book_df['ask_price1'] + book_df['bid_price1']) / 2)\n",
    "    book_df['price_spread2'] = (book_df['ask_price2'] - book_df['bid_price2']) / ((book_df['ask_price2'] + book_df['bid_price2']) / 2)\n",
    "    book_df['bid_spread'] = book_df['bid_price1'] - book_df['bid_price2']\n",
    "    book_df['ask_spread'] = book_df['ask_price1'] - book_df['ask_price2']\n",
    "    book_df[\"bid_ask_spread\"] = abs(book_df['bid_spread'] - book_df['ask_spread'])\n",
    "    book_df['total_volume'] = (book_df['ask_size1'] + book_df['ask_size2']) + (book_df['bid_size1'] + book_df['bid_size2'])\n",
    "    book_df['volume_imbalance'] = abs((book_df['ask_size1'] + book_df['ask_size2']) - (book_df['bid_size1'] + book_df['bid_size2']))\n",
    "    \n",
    "    book_df['deep_unbalance1'] = (book_df['bid_size1'] - book_df['ask_size1']) / (book_df['bid_size1'] + book_df['ask_size1'])\n",
    "    book_df['deep_unbalance2'] = (book_df['bid_size2'] - book_df['ask_size2']) / (book_df['bid_size2'] + book_df['ask_size2'])\n",
    "    \n",
    "    book_df['volume_pressure'] = book_df['bid_size1'] + book_df['bid_size2'] - book_df['ask_size1'] - book_df['ask_size2']\n",
    "    book_df['width_pressure'] = (book_df['bid_price1'] - book_df['bid_price2'] - book_df['ask_price2'] + book_df['ask_price1']) / (book_df['bid_price1'] - book_df['bid_price2'] + book_df['ask_price2'] - book_df['ask_price1'])\n",
    "    book_df['mid_price'] = (book_df['ask_price1'] + book_df['bid_price1']) / 2\n",
    "#     book_df['buy_press_fct1'] = book_df['mid_price'] / (book_df['mid_price'] - book_df['bid_price1'])\n",
    "#     book_df['buy_press_fct2'] = book_df['mid_price'] / (book_df['mid_price'] - book_df['bid_price2'])\n",
    "#     book_df['sell_press_fct1'] = book_df['mid_price'] / (book_df['ask_price1'] - book_df['mid_price'])\n",
    "#     book_df['sell_press_fct2'] = book_df['mid_price'] / (book_df['ask_price2'] - book_df['mid_price'])\n",
    "#     book_df['buy_press_sum'] = book_df['buy_press_fct1'] + book_df['buy_press_fct2']\n",
    "#     book_df['sell_press_sum'] = book_df['sell_press_fct1'] + book_df['sell_press_fct2']\n",
    "#     book_df['buy_press'] = (book_df['bid_size1'] * book_df['buy_press_fct1'] + book_df['bid_size2'] * book_df['buy_press_fct2']) / book_df['buy_press_sum']    \n",
    "#     book_df['sell_press'] = (book_df['ask_size1'] * book_df['sell_press_fct1'] + book_df['ask_size2'] * book_df['sell_press_fct2']) / book_df['sell_press_sum']\n",
    "#     book_df['order_unbalance'] = np.log(book_df['buy_press']) - np.log(book_df['sell_press'])\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum,np.std,np.mean],\n",
    "        'wap2': [np.sum,np.std,np.mean],\n",
    "        'log_return1': [realized_volatility,np.std,np.mean],\n",
    "        'log_return2': [realized_volatility,np.std,np.mean],\n",
    "        'wap_balance': [np.sum,np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        'bid_ask_spread':[np.sum, np.mean, np.std],\n",
    "        'volume_pressure': [np.sum, np.mean, np.std],\n",
    "        'deep_unbalance1': [np.sum, np.mean, np.std],\n",
    "        'deep_unbalance2': [np.sum, np.mean, np.std],\n",
    "        'width_pressure' :[np.sum, np.mean, np.std],\n",
    "        'mid_price' : [np.mean, np.std]\n",
    "#         'buy_press_fct1' : [np.mean, np.std],\n",
    "#         'buy_press_fct2' : [np.mean, np.std],\n",
    "#         'sell_press_fct1': [np.mean, np.std],\n",
    "#         'sell_press_fct2': [np.mean, np.std],\n",
    "#         'buy_press_sum':[np.mean, np.sum, np.std],\n",
    "#         'sell_press_sum':[np.mean, np.sum, np.std],\n",
    "#         'buy_press' : [np.mean, np.sum, np.std],\n",
    "#         'sell_press' : [np.mean, np.sum, np.std],\n",
    "#         'order_unbalance':[np.mean, np.sum, np.std]\n",
    "    }\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(df=book_df, create_feature_dict=create_feature_dict, seconds_in_bucket=0, add_suffix=False)\n",
    "    df_feature_500 = get_stats_window(df=book_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 500, add_suffix = True)\n",
    "#     df_feature_400 = get_stats_window(df=book_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(df=book_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 300, add_suffix = True)\n",
    "#     df_feature_200 = get_stats_window(df=book_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(df=book_df, create_feature_dict=create_feature_dict,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    \n",
    "     # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "#     df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "#     df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500', 'time_id__300','time_id__100'], axis = 1, inplace = True)\n",
    "    # Create row_id so we can merge\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(trade_df, stock_id):\n",
    "    trade_df['log_return'] = trade_df.groupby('time_id')['price'].apply(log_return)\n",
    "    trade_df['amount']= trade_df['price'] * trade_df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max,np.std,np.min],\n",
    "         'amount':[np.sum,np.max,np.min]\n",
    "    }\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 500, add_suffix = True)\n",
    "#     df_feature_400 = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 300, add_suffix = True)\n",
    "#     df_feature_200 = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(df=trade_df, create_feature_dict=create_feature_dict, seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(trade_stock):\n",
    "        # 用作 groupby 里的函数，用每笔成交价格的变化率乘以该笔成交价格的成交量，然后求和\n",
    "        price_rtn = (trade_stock['price'].diff() / trade_stock['price']) * 100\n",
    "        val = price_rtn * trade_stock['size']\n",
    "        power = val.sum()\n",
    "        return power\n",
    "    \n",
    "    def cal_mean(trade_stock):\n",
    "        return np.mean(trade_stock['price'].values)\n",
    "    \n",
    "    def cal_std(trade_stock):\n",
    "        return np.std(trade_stock['price'].values)\n",
    "    \n",
    "    def cal_singularity_price(trade_stock):\n",
    "        mean = np.mean(trade_stock['price'].values)\n",
    "        std = np.std(trade_stock['price'].values)\n",
    "        upper = mean + std\n",
    "        lower = mean - std\n",
    "        return np.sum((trade_stock['price'] >= upper) | (trade_stock['price'] <= lower))\n",
    "    \n",
    "    def cal_mean_vol(trade_stock):\n",
    "        return np.mean(trade_stock['size'].values)\n",
    "    \n",
    "    def cal_std_vol(trade_stock):\n",
    "        return np.std(trade_stock['size'].values)\n",
    "    \n",
    "    def cal_singularity_vol(trade_stock):\n",
    "        mean = np.mean(trade_stock['size'].values)\n",
    "        std = np.std(trade_stock['size'].values)\n",
    "        upper = mean + std\n",
    "        lower = mean - std\n",
    "        return np.sum((trade_stock['size'] >= upper) | (trade_stock['size'] <= lower))\n",
    "    \n",
    "    def cal_order_count_width(trade_stock):\n",
    "        return trade_stock['order_count'].max() - trade_stock['order_count'].min()\n",
    "    \n",
    "    def cal_f_max(trade_stock):\n",
    "        # 某只票在某个时间点的所有成交价格，大于平均值的数量\n",
    "        return np.sum(trade_stock['price'].values > np.mean(trade_stock['price'].values))\n",
    "    \n",
    "    def cal_f_min(trade_stock):\n",
    "        # 某只票在某个时间点的所有成交价格，小于平均值的数量\n",
    "        return np.sum(trade_stock['price'].values < np.mean(trade_stock['price'].values))\n",
    "    \n",
    "    def cal_max(trade_stock):\n",
    "        # 计算价格变化量大于0的数量\n",
    "        return np.sum(np.diff(trade_stock['price'].values) > 0)\n",
    "    \n",
    "    def cal_min(trade_stock):\n",
    "        # 计算价格变化量小于0的数量\n",
    "        return np.sum(np.diff(trade_stock['price'].values) < 0)\n",
    "    \n",
    "    def cal_abs_diff(trade_stock):\n",
    "        # 计算价格距离平均值的距离，并求距离的中位数\n",
    "        return np.median(np.abs(trade_stock['price'].values - np.mean(trade_stock['price'].values)))\n",
    "    \n",
    "    def cal_energy(trade_stock):\n",
    "        # 计算价格的平方的平均值\n",
    "        return np.mean(trade_stock['price'].values ** 2)\n",
    "    \n",
    "    def cal_iqr_p(trade_stock):\n",
    "        # 计算价格的75分位与25分位之差\n",
    "        return np.percentile(trade_stock['price'].values, 75) - np.percentile(trade_stock['price'].values, 25)\n",
    "    \n",
    "    def cal_abs_diff_v(trade_stock):\n",
    "        # 计算成交量离平均值的距离，并求距离的中位数\n",
    "        return np.median(np.abs(trade_stock['size'].values - np.mean(trade_stock['size'].values)))  \n",
    "    \n",
    "    def cal_energy_v(trade_stock):\n",
    "        # 计算成交量的平方和\n",
    "        return np.sum(trade_stock['size'].values**2)\n",
    "    \n",
    "    def iqr_p_v(trade_stock):\n",
    "        return np.percentile(trade_stock['size'].values,75) - np.percentile(trade_stock['size'].values,25)\n",
    "    \n",
    "    def price_volume_corr(trade_stock):\n",
    "        return trade_stock['price'].corr(trade_stock['size'])\n",
    "    \n",
    "    def price_volume_corr2(trade_stock):\n",
    "        diff1 = trade_stock['price'].diff()\n",
    "        diff2 = trade_stock['size'].diff()\n",
    "        return diff1.corr(diff2)\n",
    "    \n",
    "    def weighted_size(trade_stock):\n",
    "        return (trade_stock['size'] * trade_stock['order_count'] / trade_stock['order_count'].sum()).sum()\n",
    "    \n",
    "    def weighted_price(trade_stock):\n",
    "        return (trade_stock['price'] * trade_stock['order_count'] / trade_stock['order_count'].sum()).sum()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df_lr = pd.DataFrame()\n",
    "    df_lr_group = trade_df.groupby('time_id')\n",
    "    df_lr['tendency'] = df_lr_group.apply(tendency)\n",
    "    df_lr['mean'] = df_lr_group.apply(cal_mean)\n",
    "    df_lr['std'] = df_lr_group.apply(cal_std)\n",
    "    df_lr['price_singularity'] = df_lr_group.apply(cal_singularity_price)\n",
    "    df_lr['vol_mean'] = df_lr_group.apply(cal_mean_vol)\n",
    "    df_lr['vol_std'] = df_lr_group.apply(cal_std_vol)\n",
    "    df_lr['vol_singularity'] = df_lr_group.apply(cal_singularity_vol)\n",
    "    df_lr['order_count_width'] = df_lr_group.apply(cal_order_count_width)\n",
    "    df_lr['f_max'] = df_lr_group.apply(cal_f_max)\n",
    "    df_lr['f_min'] = df_lr_group.apply(cal_f_min)\n",
    "    df_lr['df_max'] = df_lr_group.apply(cal_max)\n",
    "    df_lr['df_min'] = df_lr_group.apply(cal_min)\n",
    "    df_lr['abs_diff'] = df_lr_group.apply(cal_abs_diff)\n",
    "    df_lr['energy'] = df_lr_group.apply(cal_energy)\n",
    "    df_lr['iqr_p'] = df_lr_group.apply(cal_iqr_p)\n",
    "    df_lr['abs_diff_v'] = df_lr_group.apply(cal_abs_diff_v)\n",
    "    df_lr['energy_v'] = df_lr_group.apply(cal_energy_v)\n",
    "    df_lr['iqr_p_v'] = df_lr_group.apply(iqr_p_v)\n",
    "    df_lr['pv_corr'] = df_lr_group.apply(price_volume_corr)\n",
    "    df_lr['pv_corr2'] = df_lr_group.apply(price_volume_corr2)\n",
    "    df_lr['weighted_size'] = df_lr_group.apply(weighted_size)\n",
    "    df_lr['weighted_price'] = df_lr_group.apply(weighted_price)\n",
    "    df_lr.reset_index(inplace=True)\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "#     df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "#     df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500', 'time_id__300','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37949532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', \n",
    "                'log_return1_realized_volatility_500', 'log_return2_realized_volatility_500', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_500',\n",
    "                'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_100']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parrallel for loop\n",
    "def preprocesor_help(stock_id, is_train, cache):\n",
    "    # Train\n",
    "    t0 = dt.datetime.now()\n",
    "    if is_train:\n",
    "        file_path_book = root_path + \"/book_train.parquet/stock_id=\" + str(stock_id)\n",
    "        file_path_trade = root_path + \"/trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "    # Test\n",
    "    else:\n",
    "        file_path_book = root_path + \"/book_test.parquet/stock_id=\" + str(stock_id)\n",
    "        file_path_trade = root_path + \"/trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "\n",
    "    # Preprocess book and trade data and merge them\n",
    "    book_df = pd.read_parquet(file_path_book)\n",
    "    trade_df = pd.read_parquet(file_path_trade)\n",
    "    book_stock_proprocess = book_preprocessor(book_df, str(stock_id))\n",
    "    trade_stock_proprocess = trade_preprocessor(trade_df, str(stock_id))\n",
    "    df_tmp = pd.merge(book_stock_proprocess, trade_stock_proprocess, on = 'row_id', how = 'left')\n",
    "    t1 = dt.datetime.now()\n",
    "    print(\"stock_id: {} finishes, total time: {}\".format(stock_id, ((t1-t0).total_seconds())))\n",
    "    cache.append(df_tmp)\n",
    "\n",
    "\n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    # Use parallel api to call paralle for loop\n",
    "#     df = Parallel(n_jobs = -1, verbose = 1)(delayed(preprocesor_help)(stock_id) for stock_id in list_stock_ids)\n",
    "    pool = Pool(4)\n",
    "    cache = multiprocessing.Manager().list()\n",
    "#     cache = []\n",
    "    for stock_id in list_stock_ids:\n",
    "        pool.apply_async(preprocesor_help, args=(stock_id, is_train, cache, ))\n",
    "#         preprocesor_help(stock_id, True, cache)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    cache_list = list(cache)\n",
    "#     cache_list = cache\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(cache_list, ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d133d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fa164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f00e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "del train_\n",
    "del test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_500'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_500'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_100'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_100'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e798cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from numpy.random import seed\n",
    "seed(114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将train里的特征用quantile 进行变化\n",
    "colNames = list(train)\n",
    "colNames.remove('time_id')\n",
    "colNames.remove('target')\n",
    "colNames.remove('row_id')\n",
    "colNames.remove('stock_id')\n",
    "\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab408b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "train_p.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "train_p.iloc[:, [35,70,88, 66, 12]] = train_p.iloc[:, [35,70,88, 66, 12]].ffill()\n",
    "train_p.dropna(axis=1, inplace=True, how='any')\n",
    "corr = train_p.corr()\n",
    "ids = corr.index\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(train_p.T.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22afc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for n in range(7):  # 获取每一类有哪几个index\n",
    "    l.append (ids[cat_labels == n].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df44d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = []\n",
    "matTest = []\n",
    "n = 0\n",
    "for ind in l:\n",
    "    # 把kmeans 分类出来的结果都存下来，然后按time_id 取feature的平均值，最后的结果是在同一时间，这一类所有股票的\n",
    "    # feature 平均值\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind)]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf ) \n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_concat_train = pd.concat(mat).reset_index()\n",
    "stk_concat_train.drop(columns=['target'],inplace=True)\n",
    "stk_concat_test = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838deb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_concat_test = pd.concat([stk_concat_test, stk_concat_train.loc[stk_concat_train.time_id == 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有的feature 的名称加上后缀，后缀为kmeans 分出来的类\n",
    "stk_pivot_train = stk_concat_train.pivot(index='time_id', columns='stock_id')\n",
    "stk_pivot_train.columns = [\"_\".join(x) for x in stk_pivot_train.columns.ravel()]\n",
    "stk_pivot_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec729cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有的feature 的名称加上后缀，后缀为kmeans 分出来的类\n",
    "stk_pivot_test = stk_concat_test.pivot(index='time_id', columns='stock_id')\n",
    "stk_pivot_test.columns = [\"_\".join(x) for x in stk_pivot_test.columns.ravel()]\n",
    "stk_pivot_test.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1',\n",
    "     'deep_unbalance1_mean_0c1',\n",
    "     'deep_unbalance1_mean_1c1',\n",
    "     'deep_unbalance1_mean_2c1',\n",
    "     'deep_unbalance1_mean_3c1',\n",
    "     'deep_unbalance1_mean_4c1',\n",
    "     'deep_unbalance1_mean_6c1']\n",
    "# 把用kmeans 分类出来的信息按照 time_id, merge到原train\n",
    "train_merge = pd.merge(train, stk_pivot_train[nnn], how='left',on='time_id')\n",
    "test_merge = pd.merge(test, stk_pivot_test[nnn], how='left',on='time_id')\n",
    "del book_train\n",
    "del book_test\n",
    "del trade_train\n",
    "del trade_test\n",
    "del train_p\n",
    "del stk_pivot_train\n",
    "del stk_pivot_test\n",
    "del stk_concat_train\n",
    "del stk_concat_test\n",
    "del train\n",
    "del test\n",
    "del mat\n",
    "del matTest\n",
    "del newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "tf.random.set_seed(114)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train, useful_features, categoricals, target, n_splits, num_leaves, max_depth, min_child_weight, feature_fraction, lambda_l1, lambda_l2, \n",
    "          bagging_fraction, min_data_in_leaf, learning_rate, n_estimators):\n",
    "     \n",
    "    params =  {'num_leaves': int(num_leaves),  \n",
    "        'max_depth' : int(max_depth),\n",
    "       'min_child_weight': min_child_weight,\n",
    "       'feature_fraction': feature_fraction,\n",
    "       'bagging_fraction': bagging_fraction,\n",
    "       'min_data_in_leaf': int(min_data_in_leaf), \n",
    "       'objective': 'regression',\n",
    "       \"metric\": 'rmse',\n",
    "       'learning_rate': learning_rate, \n",
    "       \"boosting_type\": \"gbdt\",\n",
    "       \"bagging_seed\": 11,\n",
    "       \"verbosity\": -1,\n",
    "       'random_state': 46,\n",
    "       'lambda_l1': lambda_l1,  \n",
    "       'lambda_l2': lambda_l2, \n",
    "       'n_estimators': int(n_estimators),\n",
    "       'early_stopping': 150,\n",
    "       'n_jobs': -1       \n",
    "}\n",
    "\n",
    "    def run_lgb(reduce_train, useful_features, categoricals, target, n_splits = n_splits):\n",
    "        #useful_features.remove('installation_id')\n",
    "        rmse_score_list = []\n",
    "        kf = model_selection.KFold(n_splits = n_splits, random_state = 42, shuffle = True)\n",
    "        oof_predict = np.zeros((len(reduce_train), ))\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(reduce_train)):\n",
    "            X_train = reduce_train[useful_features].iloc[train_index]\n",
    "            X_val = reduce_train[useful_features].iloc[test_index]\n",
    "            y_train = reduce_train[target].iloc[train_index]\n",
    "            y_val = reduce_train[target].iloc[test_index]\n",
    "\n",
    "            train_weights = 1 / np.square(y_train)\n",
    "            val_weights = 1 / np.square(y_val)\n",
    "            train_dataset = lgb.Dataset(X_train, y_train, weight = train_weights, categorical_feature = [categoricals])\n",
    "            val_dataset = lgb.Dataset(X_val, y_val, weight = val_weights, categorical_feature = [categoricals])\n",
    "\n",
    "            lgb_model = lgb.train(params, train_dataset, num_boost_round = params['n_estimators'], \n",
    "                                  valid_sets = [train_dataset, val_dataset],\n",
    "                                 early_stopping_rounds = params['early_stopping'],\n",
    "                                 feval = feval_rmspe, verbose_eval=False)\n",
    "\n",
    "            val_predict = lgb_model.predict(X_val)\n",
    "             # Add predictions to the out of folds array\n",
    "            oof_predict[test_index] = val_predict\n",
    "\n",
    "        rmspe_score = rmspe(reduce_train['target'], oof_predict)\n",
    "        return -rmspe_score\n",
    "\n",
    "    return run_lgb(train, useful_features, categoricals, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = list(train_merge)\n",
    "useful_features.remove('row_id')\n",
    "useful_features.remove('target')\n",
    "useful_features.remove('time_id')\n",
    "target = 'target'\n",
    "bounds_LGB = {\n",
    "    'num_leaves' : (50, 100),\n",
    "    'max_depth': (2, 80),\n",
    "    'min_child_weight' : (0.01, 0.6),\n",
    "    'min_data_in_leaf' : (80, 1000),\n",
    "    'feature_fraction' : (0.1, 0.95),\n",
    "    'lambda_l1': (0, 10),\n",
    "    'lambda_l2': (0, 10),\n",
    "    'bagging_fraction': (0.2, 1),\n",
    "    'learning_rate': (0.01, 0.8),\n",
    "    'n_estimators' : (500,7000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d36179",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c30ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每一组训练一次模型，用lightgbm 和 Bayesian 优化\n",
    "init_points = 12\n",
    "n_iter = 12 \n",
    "best_param = {}\n",
    "for idx, indiv in enumerate(l):\n",
    "    kmeans_stock = train_merge.loc[train_merge['stock_id'].isin(indiv)]\n",
    "    kmeans_stock.reset_index(inplace=True, drop=True)\n",
    "    partial_model = partial(model, kmeans_stock, useful_features, categoricals='stock_id',target=target, n_splits = 5)\n",
    "    LGB_BO = BayesianOptimization(partial_model, bounds_LGB, random_state = 1029)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')   \n",
    "        LGB_BO.maximize(init_points = init_points, n_iter = n_iter, acq='ei', alpha=1e-6)\n",
    "    best_param[idx] = LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_transform(params):\n",
    "    params_new =  {'num_leaves': int(params['num_leaves']),  \n",
    "            'max_depth' : int(params['max_depth']),\n",
    "           'min_child_weight': params['min_child_weight'],\n",
    "           'feature_fraction': params['feature_fraction'],\n",
    "           'bagging_fraction': params['bagging_fraction'] ,\n",
    "           'min_data_in_leaf': int(params['min_data_in_leaf']), \n",
    "           'objective': 'regression',\n",
    "           \"metric\": 'rmse',   \n",
    "           'learning_rate': params['learning_rate'], \n",
    "           \"boosting_type\": \"gbdt\",\n",
    "           \"bagging_seed\": 11,\n",
    "           \"verbosity\": -1,\n",
    "           'random_state': 46,\n",
    "           'lambda_l1': params['lambda_l1'],  \n",
    "           'lambda_l2': params['lambda_l2'], \n",
    "           'n_estimators': int(params['n_estimators']),\n",
    "           'early_stopping': 150,\n",
    "           'n_jobs': -1       \n",
    "    }\n",
    "    return params_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ca86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lgb(train_data, test_data, params):\n",
    "    model_name = 'NN'\n",
    "    n_folds = 5\n",
    "    counter = 1\n",
    "    target_name = 'target'\n",
    "    validation_scores = []\n",
    "    test_result = {}\n",
    "    oof_predictions = np.zeros(train_data.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test_data.shape[0])\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "    features_to_consider = list(train_data)\n",
    "    features_to_consider.remove('time_id')\n",
    "    features_to_consider.remove('target')\n",
    "    features_to_consider.remove('row_id')\n",
    "    num_features = features_to_consider.copy()\n",
    "    num_features.remove('stock_id')\n",
    "    train_data[num_features] = train_data[num_features].fillna(train_data[num_features].mean())\n",
    "    test_data[num_features] = test_data[num_features].fillna(test_data[num_features].mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "        X_train = train_data.loc[train_idx, features_to_consider]\n",
    "        y_train = train_data.loc[train_idx, target_name]\n",
    "        X_test = train_data.loc[val_idx, features_to_consider]\n",
    "        y_test = train_data.loc[val_idx, target_name]\n",
    "        \n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_test)\n",
    "        train_dataset = lgb.Dataset(X_train, y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(X_test, y_test, weight = val_weights)\n",
    "        \n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round = 1300,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        \n",
    "         # Add predictions to the out of folds array\n",
    "        oof_predictions[val_idx] = model.predict(X_test)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test_data[features_to_consider]) / 5\n",
    "#         break\n",
    "    rmspe_score = rmspe(train_data['target'], oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')  \n",
    "    lgb.plot_importance(model, max_num_features=30)\n",
    "    return test_predictions\n",
    "\n",
    "test_predict_list = []\n",
    "train_predict_list = []\n",
    "\n",
    "\n",
    "for idx, indiv in enumerate(l):\n",
    "    kmeans_stock = train_merge.loc[train_merge['stock_id'].isin(indiv)]\n",
    "    kmeans_stock_test = test_merge.loc[test_merge['stock_id'].isin(indiv)]\n",
    "    train_predictions = np.zeros(kmeans_stock.shape[0])\n",
    "    test_predictions = np.zeros(kmeans_stock_test.shape[0])\n",
    "    kf = model_selection.KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    params = best_param[idx]\n",
    "    params = param_transform(params)\n",
    "    kmeans_stock_test_valid = kmeans_stock_test[useful_features]\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(kmeans_stock)):\n",
    "        X_train = kmeans_stock[useful_features].iloc[train_index]\n",
    "        X_val = kmeans_stock[useful_features].iloc[test_index]\n",
    "        y_train = kmeans_stock[target].iloc[train_index]\n",
    "        y_val = kmeans_stock[target].iloc[test_index]\n",
    "\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(X_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(X_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        lgb_model = lgb.train(params, train_dataset, num_boost_round = params['n_estimators'], \n",
    "                              valid_sets = [train_dataset, val_dataset],\n",
    "                             early_stopping_rounds = params['early_stopping'],\n",
    "                             feval = feval_rmspe, verbose_eval=False)\n",
    "        \n",
    "        try:\n",
    "            train_predictions[test_index] += lgb_model.predict(X_val)\n",
    "            test_predictions += lgb_model.predict(kmeans_stock_test_valid) / 5\n",
    "        except:\n",
    "            test_predictions = np.zeros(kmeans_stock_test.shape[0])\n",
    "    kmeans_stock['target_pred'] = train_predictions\n",
    "    kmeans_stock_test['target'] = test_predictions\n",
    "    train_predict_list.append(kmeans_stock)\n",
    "    test_predict_list.append(kmeans_stock_test)\n",
    "    \n",
    "test_submit = pd.concat(test_predict_list)\n",
    "train_pred = pd.concat(train_predict_list)\n",
    "train_pred.sort_values('row_id', inplace=True)\n",
    "test_submit.sort_values('row_id', inplace=True)\n",
    "\n",
    "del test_predict_list\n",
    "del kmeans_stock\n",
    "del kmeans_stock_test\n",
    "del test_predictions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': swish})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colNames:\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_merge[col] = qt.fit_transform(train_merge[[col]])\n",
    "    test_merge[col] = qt.transform(test_merge[[col]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = (252, 128, 64, 32)\n",
    "stock_embedding_size = 32\n",
    "cat_data = train_merge['stock_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe23ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_dim = train_merge.shape[1] - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')  # stock id input, 维数是1维\n",
    "    num_input = keras.Input(shape=(num_data_dim,), name='num_data')  # num_data input, 维数是 266维\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)  #  降维，将（none, 24, 1）,降成 （24,1）\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input]) # shape (None, 386), 其中num_input shape 为 (362， )\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        out = keras.layers.Dropout(rate=0.02,seed=111)(out)\n",
    "        \n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "def train_with_nn(train_data, test_data):\n",
    "    model_name = 'NN'\n",
    "    n_folds = 5\n",
    "    counter = 1\n",
    "    target_name = 'target'\n",
    "    validation_scores = []\n",
    "    oob_predictions = np.zeros(train_data.shape[0])\n",
    "    test_result = {}\n",
    "    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "    features_to_consider = list(train_data)\n",
    "    features_to_consider.remove('time_id')\n",
    "    features_to_consider.remove('target')\n",
    "    features_to_consider.remove('row_id')\n",
    "    num_features = features_to_consider.copy()\n",
    "    num_features.remove('stock_id')\n",
    "    train_data[num_features] = train_data[num_features].fillna(train_data[num_features].mean())\n",
    "    test_data[num_features] = test_data[num_features].fillna(test_data[num_features].mean())\n",
    "    test_data['target'] = 0\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))  \n",
    "    \n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "        X_train = train_data.loc[train_idx, features_to_consider]\n",
    "        y_train = train_data.loc[train_idx, target_name]\n",
    "        X_test = train_data.loc[val_idx, features_to_consider]\n",
    "        y_test = train_data.loc[val_idx, target_name]\n",
    "        model = base_model()\n",
    "        model.compile(\n",
    "            keras.optimizers.Adam(learning_rate=0.0061),\n",
    "            loss=root_mean_squared_per_error\n",
    "        )\n",
    "        \n",
    "        num_data_train = X_train[num_features]\n",
    "        num_data_val = X_test[num_features]       \n",
    "        num_data_train = scaler.fit_transform(num_data_train.values) \n",
    "        num_data_val = scaler.transform(num_data_val.values)\n",
    "        cat_data_train = X_train['stock_id']\n",
    "        cat_data_val = X_test['stock_id']\n",
    "        \n",
    "        model.fit([cat_data_train, num_data_train], \n",
    "              y_train,               \n",
    "              batch_size=2048,\n",
    "              epochs=200,\n",
    "              validation_data=([cat_data_val, num_data_val], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "              verbose = 1)\n",
    "\n",
    "        preds = model.predict([cat_data_val, num_data_val]).reshape(1,-1)[0]\n",
    "        oob_predictions[val_idx] = preds\n",
    "        score = round(rmspe(y_true = y_test, y_pred = preds), 5)\n",
    "        print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "        validation_scores.append(score)\n",
    "        \n",
    "        test_data_transformed = scaler.transform(test_data[num_features].values)\n",
    "        test_score = model.predict([test_data['stock_id'], test_data_transformed]).reshape(1,-1)[0].clip(0,1e10)\n",
    "        print(\"current test score is: {}\".format(test_score))\n",
    "        test_data['target'] += (test_score / n_folds)\n",
    "        del test_score\n",
    "#         test_result[fold] = (test_score)\n",
    "#         break\n",
    "    return oob_predictions, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_predictions, test_data = train_with_nn(train_merge, test_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge['pred'] = oob_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5128bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge.sort_values('row_id', inplace=True)\n",
    "test_merge.sort_values('row_id', inplace=True)\n",
    "lambda_bound = {'lambda1': (0,1)}\n",
    "def result_optimizer(lambda1):\n",
    "    result = lambda1 * train_merge['pred'].values + (1 - lambda1) * train_pred['target_pred'].values\n",
    "    return -round(rmspe(y_true = train_pred['target'].values, y_pred = result), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.1702\n",
    "# pred_lgb_m = predictions_lgb * 0.615 + predictions_lgb2 * 0.385   \n",
    "test_data['target'] = test_data['target'] * lambda1 + test_submit['target'] * (1-lambda1) \n",
    "test_data[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
